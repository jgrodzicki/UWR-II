{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zAiECkYEaZn1"
   },
   "source": [
    "# Assignment 1\n",
    "\n",
    "## Important notes\n",
    "**Submission deadline:**\n",
    "* **Thursday, 12.03.2020**\n",
    "\n",
    "**Points: 13 + 2bp**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bb0zN1GvapSt"
   },
   "source": [
    "This assignment is meant to test your skills in course pre-requisites:  Scientific Python programming and  Machine Learning. If it is hard, I strongly advise you to drop the course.\n",
    "\n",
    "Please use GitHub’s [pull requests](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/about-pull-requests) and issues to send corrections!\n",
    "\n",
    "You can solve the assignment in any system you like, but we encourage you to try out [Google Colab](https://colab.research.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mglearn.tools import visualize_coefficients\n",
    "\n",
    "import re\n",
    "import scipy.optimize as sopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCountVectorizer():\n",
    "    \n",
    "    def __init__(self, neg_words=[], enh_words=[], enh_mult=2, no_words=None):\n",
    "        self.neg_words = neg_words\n",
    "        self.enh_words = enh_words\n",
    "        self.enh_mult = enh_mult\n",
    "        self.no_words = no_words\n",
    "    \n",
    "    def fit(self, sentences):\n",
    "        count = defaultdict(int)\n",
    "        for s in sentences:\n",
    "            for w in s.split():\n",
    "                count[w] += 1\n",
    "        \n",
    "        srt = sorted(count.items(), key=lambda kv: kv[1], reverse=True)\n",
    "        if self.no_words is not None:\n",
    "            srt = srt[:self.no_words]\n",
    "        \n",
    "        self.idx_vocabulary = {w:i for i, (w, c) in enumerate(srt)}\n",
    "    \n",
    "    def transform(self, sentences):\n",
    "        vect = np.zeros((len(sentences), len(self.idx_vocabulary)))\n",
    "\n",
    "        for i, s in enumerate(sentences):\n",
    "            neg = 1\n",
    "            enh = 1\n",
    "            \n",
    "            for w in s.split():\n",
    "                if w not in self.idx_vocabulary:\n",
    "                    continue\n",
    "                \n",
    "                idx = self.idx_vocabulary[w]\n",
    "                \n",
    "                if w in self.neg_words:\n",
    "                    neg *= -1\n",
    "                elif w in self.enh_words:\n",
    "#                     enh += self.enh_mult\n",
    "                    enh *= self.enh_mult\n",
    "                else:\n",
    "                    vect[i][idx] += neg * enh\n",
    "        \n",
    "        return vect\n",
    "    \n",
    "    def fit_transform(self, sentences):\n",
    "        self.fit(sentences)\n",
    "        return self.transform(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression:\n",
    "    \n",
    "    def __init__(self, neg_words=[], enh_words=[], no_words=None, enh_mult=2,\n",
    "                 params=None, max_iter=100, solver=sopt.fmin_l_bfgs_b):\n",
    "        self.neg_words = neg_words\n",
    "        self.enh_words = enh_words\n",
    "        self.no_words = no_words\n",
    "        self.enh_mult = enh_mult\n",
    "        self.params = params\n",
    "        self.max_iter = max_iter\n",
    "        self.solver = solver\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def logreg_loss(self, params, sentences, Y):\n",
    "        params = params.astype(np.float64)\n",
    "        Y = Y.astype(np.float64)\n",
    "        \n",
    "#         N, E = params[:2]\n",
    "#         Theta = params[2:]\n",
    "        \n",
    "        cv = CustomCountVectorizer(neg_words=self.neg_words, enh_words=self.enh_words,\n",
    "                                  enh_mult=self.enh_mult, no_words=self.no_words)\n",
    "        X = cv.fit_transform(sentences)\n",
    "        X = np.hstack((np.ones((X.shape[0], 3)), X))  # bias, N, E\n",
    "        X = X.astype(np.float64)\n",
    "        \n",
    "        Z = np.dot(X, params.T)\n",
    "\n",
    "        sig_Z = self.sigmoid(Z)\n",
    "        Y_ = Y[:,np.newaxis]\n",
    "        nll = -np.sum((Y_ * np.log2(sig_Z + 1e-50) + (1-Y_) * np.log2(1 - sig_Z + 1e-50)))\n",
    "        nll += np.sum(params**2) / 2\n",
    "        \n",
    "        grad = np.dot(X.T, (sig_Z-Y).T)\n",
    "        grad = grad.reshape(params.shape) + params\n",
    "\n",
    "        return nll / len(Y), grad / len(Y)\n",
    "    \n",
    "    def fit(self, sentences, Y):\n",
    "        params = self.params\n",
    "        cv = CustomCountVectorizer(neg_words=self.neg_words, enh_words=self.enh_words,\n",
    "                                  no_words=self.no_words, enh_mult=self.enh_mult)\n",
    "        X = cv.fit_transform(sentences)\n",
    "        \n",
    "        if params is None:\n",
    "            params = np.ones(X.shape[1] + 3)\n",
    "        \n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        params = self.solver(lambda p: self.logreg_loss(p, sentences, Y), \n",
    "                             params,\n",
    "                             maxiter=self.max_iter)[0]\n",
    "        self.params = params\n",
    "        \n",
    "    def predict(self, sentences):\n",
    "        self.N, self.E = self.params[:2]\n",
    "        Theta = self.params[2:]\n",
    "        cv = CustomCountVectorizer(neg_words=self.neg_words, enh_words=self.enh_words,\n",
    "                                  no_words=self.no_words, enh_mult=self.enh_mult)\n",
    "        X = cv.fit_transform(sentences)\n",
    "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "        \n",
    "        preds = np.dot(Theta, X.T)\n",
    "        return preds, self.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qIJIJDwgZrOI"
   },
   "source": [
    "1. **[1p]** Download data competition from a Kaggle competition on sentiment prediction from [[https://www.kaggle.com/c/CountVectorizerent-analysis-on-movie-reviews/data](https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data)].  Keep only full sentences, i.e. for each `SenteceId` keep only the entry with the lowest `PhraseId`.  Use first 7000 sentences as a `train set` and the remaining 1529 sentences as the `test set`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(s):\n",
    "#     return re.sub('\\ +\\W\\ +', ' ', s.lower())\n",
    "    return re.sub('[^\\w\\s]', '', s.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.tsv', sep='\\t')\n",
    "df = df.groupby(['SentenceId'], as_index=False).agg({'PhraseId' : 'min',\n",
    "                                               'Phrase' : 'first',\n",
    "                                               'Sentiment' : 'first'})\n",
    "df['Sentiment'] /= 4\n",
    "df = df.drop(['PhraseId', 'SentenceId'], axis=1)\n",
    "df['Phrase'] = df['Phrase'].apply(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = df.iloc[:7000]\n",
    "test_df = df.iloc[7000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a series of escapades demonstrating the adage ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>this quiet  introspective and entertaining ind...</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>even fans of ismail merchant s work  i suspect...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a positively thrilling combination of ethnogra...</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aggressive selfglorification and a manipulativ...</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Phrase  Sentiment\n",
       "0  a series of escapades demonstrating the adage ...       0.25\n",
       "1  this quiet  introspective and entertaining ind...       1.00\n",
       "2  even fans of ismail merchant s work  i suspect...       0.25\n",
       "3  a positively thrilling combination of ethnogra...       0.75\n",
       "4  aggressive selfglorification and a manipulativ...       0.25"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **[1p]** Prepare the data for logistic regression:\n",
    "\tMap the sentiment scores $0,1,2,3,4$ to a probability of the sentence being by setting $p(\\textrm{positive}) = \\textrm{sentiment}/4$.\n",
    "\tBuild a dictionary of most frequent 20000 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7000/7000 [00:01<00:00, 6186.28it/s]\n"
     ]
    }
   ],
   "source": [
    "word_count = defaultdict(int)\n",
    "for i in trange(len(train_df), position=0, leave=True):\n",
    "    for w in train_df.iloc[i]['Phrase'].split():\n",
    "        word_count[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict(sorted(word_count.items(), key=lambda kv: kv[1], reverse=True)[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **[3p]** Treat each document as a bag of words. e.g. if the vocabulary is \n",
    "\t```\n",
    "\t0: the\n",
    "\t1: good\n",
    "\t2: movie\n",
    "\t3: is\n",
    "\t4: not\n",
    "\t5: a\n",
    "\t6: funny\n",
    "\t```\n",
    "\tThen the encodings can be:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,1,0,0,1,0,0] \n",
    "\tthe movie is not a funny movie: [1,0,2,1,1,1,1]\n",
    "\t```\n",
    "    Train a logistic regression model to predict the sentiment. Compute the correlation between the predicted probabilities and the sentiment. Record the most positive and negative words.\n",
    "    Please note that in this model each word gets its sentiment parameter $S_w$ and the score for a sentence is \n",
    "    $$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}S_w$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_df['Sentiment']\n",
    "test_y = test_df['Sentiment']\n",
    "\n",
    "cv = CustomCountVectorizer()\n",
    "train_x = cv.fit_transform(train_df['Phrase'])\n",
    "test_x = cv.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 7s, sys: 3.94 s, total: 2min 11s\n",
      "Wall time: 56.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf.fit(train_x, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 838   26   11    3    0]\n",
      " [   5 1779   17    9    0]\n",
      " [   2   33 1300   24    3]\n",
      " [   2   20   13 1852   15]\n",
      " [   1    8   14   25 1000]]\n",
      "Accuracy for train: 0.967\n",
      "Accuracy for test: 0.39110529758011775\n"
     ]
    }
   ],
   "source": [
    "preds_tr = clf.predict(train_x)\n",
    "\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf.predict(test_x)).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative words: ['dull' 'terrible' 'devoid' 'stupid' 'worst']\n",
      "Most positive words: ['remarkable' 'captivating' 'best' 'beautifully' 'masterpiece']\n"
     ]
    }
   ],
   "source": [
    "print(f'Most negative words: {np.array(list(cv.idx_vocabulary.keys()))[clf.coef_[0].argsort()[-5:]]}')\n",
    "print(f'Most positive words: {np.array(list(cv.idx_vocabulary.keys()))[clf.coef_[-1].argsort()[-5:]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 2000 most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CustomCountVectorizer(no_words=2000)\n",
    "train_x = cv.fit_transform(train_df['Phrase'])\n",
    "test_x = cv.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.4 s, sys: 1.31 s, total: 24.7 s\n",
      "Wall time: 8.61 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf.fit(train_x, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 569  174   67   58   10]\n",
      " [  51 1444  146  146   23]\n",
      " [  30  219  903  182   28]\n",
      " [  22  161  120 1506   93]\n",
      " [   8   54   46  181  759]]\n",
      "Accuracy for train: 0.7401428571428571\n",
      "Accuracy for test: 0.3590582079790713\n"
     ]
    }
   ],
   "source": [
    "preds_tr = clf.predict(train_x)\n",
    "\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf.predict(test_x)).mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most negative words: ['dull' 'horrible' 'stupid' 'devoid' 'worst']\n",
      "Most positive words: ['entertaining' 'beautifully' 'dazzling' 'remarkable' 'masterpiece']\n"
     ]
    }
   ],
   "source": [
    "print(f'Most negative words: {np.array(list(cv.idx_vocabulary.keys()))[clf.coef_[0].argsort()[-5:]]}')\n",
    "print(f'Most positive words: {np.array(list(cv.idx_vocabulary.keys()))[clf.coef_[-1].argsort()[-5:]]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **[3p]** Now prepare an encoding in which negation flips the sign of the following words. For instance for our vocabulary the encodings become:\n",
    "\t```\n",
    "\tgood:                           [0,1,0,0,0,0,0]\n",
    "\tnot good:                       [0,-1,0,0,1,0,0]\n",
    "\tnot not good:                   [0,1,0,0,0,0,0]\n",
    "\tthe movie is not a funny movie: [1,0,0,1,1,-1,-1]\n",
    "\t```\n",
    "\tFor best results, you will probably need to construct a list of negative words.\n",
    "\t\n",
    "\tAgain train a logistic regression classifier and compare the results to the Bag of Words approach.\n",
    "\t\n",
    "\tPlease note that this model still maintains a single parameter for each word, but now the sentence score is\n",
    "\t$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}-1^{\\text{count of negations preceeding }w}S_w$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_words = ['no', 'none', 'not', 'never', 'nobody']\n",
    "# neg_words = ['no']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for full dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CustomCountVectorizer(neg_words=neg_words)\n",
    "train_x2 = cv2.fit_transform(train_df['Phrase'])\n",
    "test_x2 = cv2.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 59s, sys: 3.61 s, total: 2min 2s\n",
      "Wall time: 51.2 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf2 = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf2.fit(train_x2, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 831   32    6    8    1]\n",
      " [   2 1779   18   11    0]\n",
      " [   2   38 1298   21    3]\n",
      " [   5   15   17 1852   13]\n",
      " [   2   12   13   26  995]]\n",
      "Accuracy for train: 0.965\n",
      "Accuracy for test: 0.37933289731850883\n"
     ]
    }
   ],
   "source": [
    "preds_tr2 = clf2.predict(train_x2)\n",
    "\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr2)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr2).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf2.predict(test_x2)).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for 2000 most popular words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2 = CustomCountVectorizer(neg_words=neg_words, no_words=2000)\n",
    "train_x2 = cv2.fit_transform(train_df['Phrase'])\n",
    "test_x2 = cv2.transform(test_df['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.7 s, sys: 1.25 s, total: 24 s\n",
      "Wall time: 8.31 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='multinomial', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "clf2 = LogisticRegression(multi_class='multinomial', max_iter=1000, solver='lbfgs')\n",
    "clf2.fit(train_x2, train_y*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n",
      "[[ 537  186   62   80   13]\n",
      " [  58 1422  113  187   30]\n",
      " [  42  253  845  190   32]\n",
      " [  31  174  128 1484   85]\n",
      " [  12   63   55  176  742]]\n",
      "Accuracy for train: 0.7185714285714285\n",
      "Accuracy for test: 0.355134074558535\n"
     ]
    }
   ],
   "source": [
    "preds_tr2 = clf2.predict(train_x2)\n",
    "\n",
    "print(f'Confusion matrix:\\n{confusion_matrix(train_y*4, preds_tr2)}')\n",
    "print(f'Accuracy for train: {(train_y*4 == preds_tr2).mean()}')\n",
    "print(f'Accuracy for test: {(test_y*4 == clf2.predict(test_x2)).mean()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **[5p]** Now also consider emphasizing words such as `very`. They can boost (multiply by a constant >1) the following words.\n",
    "\tImplement learning the modifying multiplier for negation and for emphasis. One way to do this is to introduce a model which has:\n",
    "\t- two modifiers, $N$ for negation and $E$ for emphasis\n",
    "\t- a sentiment score $S_w$ for each word \n",
    "And score each sentence as:\n",
    "$$\\text{score}(\\text{sentence}) = \\sum_{w\\text{ in sentence}}N^{\\text{#negs prec. }w}E^{\\text{#emphs prec. }w}S_w$$\n",
    "\n",
    "You will need to implement a custom logistic regression model to support it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "enh_words = ['very', 'extremely', 'exceedingly', 'exceptionally', 'especially', 'tremendously', 'immensely', 'vastly', 'hugely', 'extraordinarily', 'extra', 'excessively', 'overly', 'over', 'abundantly', 'inordinately', 'singularly', 'significantly', 'distinctly', 'outstandingly', 'uncommonly', 'unusually', 'decidedly', 'particularly', 'eminently', 'supremely', 'highly', 'remarkably', 'really', 'truly', 'mightily', 'thoroughly', 'all that', 'to a great extent', 'most', 'so', 'too', 'unco', 'très', 'right', 'terrifically', 'awfully', 'terribly', 'devilishly', 'madly', 'majorly', 'seriously', 'desperately', 'mega', 'ultra', 'oh-so', 'too-too', 'stinking', 'mucho', 'damn', 'damned', 'too … for words', 'devilish', 'hellish', 'frightfully', 'ever so', 'well', 'bloody', 'dead', 'dirty', 'jolly', 'fair', 'real', 'mighty', 'powerful', 'awful', 'plumb', 'darned', 'way', 'bitching', 'mad', 'lekker', 'exceeding', 'sore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jakubgrodzicki/.pyenv/versions/3.7.3/lib/python3.7/site-packages/ipykernel_launcher.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 6min 40s, sys: 3min 56s, total: 10min 36s\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clf3 = CustomLogisticRegression(neg_words=neg_words, enh_words=enh_words, max_iter=200)\n",
    "clf3.fit(train_df['Phrase'], train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf3.predict(train_df['Phrase'])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49328571428571427"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.round(4*preds)/4 == train_y).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. **[2pb]** Propose, implement, and evaluate an extension to the above model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
