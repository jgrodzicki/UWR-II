{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from enum import Enum\n",
    "from itertools import permutations\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, NamedTuple, Optional, Tuple, DefaultDict\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Author(Enum):\n",
    "    PRUS = 0\n",
    "    SIENKIEWICZ = 1\n",
    "    ORZESZKOWA = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'\\W', ' ', text)\n",
    "    text = re.sub(r'\\ +', ' ', text)\n",
    "    return text\n",
    "\n",
    "def load_corpus_list(path: str) -> List[str]:\n",
    "    text = open(path, 'r').read()\n",
    "    preprocessed = preprocess(text)\n",
    "    words = preprocessed.split()\n",
    "    return words\n",
    "\n",
    "def occurence_dict_from_corpus(words: List[str]) -> Dict[str, float]:\n",
    "    unique_words, word_counts = np.unique(words, return_counts=True)\n",
    "    word_counts_perc = word_counts / np.sum(word_counts)\n",
    "    return defaultdict(float, zip(unique_words, word_counts_perc))\n",
    "\n",
    "def create_occurence_dicts() -> Dict[Author, Dict[str, float]]:\n",
    "    occurence_dicts = dict()\n",
    "    occurence_dicts[Author.PRUS] = occurence_dict_from_corpus(\n",
    "        words=load_corpus_list(path='data.nogit/korpus_prusa.txt')\n",
    "    )\n",
    "    occurence_dicts[Author.SIENKIEWICZ] = occurence_dict_from_corpus(\n",
    "        words=load_corpus_list(path='data.nogit/korpus_sienkiewicza.txt')\n",
    "    )\n",
    "    occurence_dicts[Author.ORZESZKOWA] = occurence_dict_from_corpus(\n",
    "        words=load_corpus_list(path='data.nogit/korpus_orzeszkowej.txt')\n",
    "    )\n",
    "    return occurence_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_word_to_tag_mapping(path: str) -> Dict[str, str]:\n",
    "    file = open(path, 'r').read()\n",
    "    return dict(np.array(file.replace('\\n', ' ').split()).reshape(-1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bigrams(\n",
    "    path: str,\n",
    "    word_to_tag_mapping: Dict[str, str],\n",
    ") -> Tuple[\n",
    "        DefaultDict[str, List[str]],\n",
    "        DefaultDict[Tuple[str, str], float],\n",
    "        DefaultDict[str, List[str]],\n",
    "        DefaultDict[Tuple[str, str], float],\n",
    "    ]:\n",
    "    MIN_N = 2\n",
    "\n",
    "    word_mapping = defaultdict(set)\n",
    "    word_mapping_count = defaultdict(float)\n",
    "    tag_mapping = defaultdict(set)\n",
    "    tag_mapping_count = defaultdict(float)\n",
    "    \n",
    "    pbar = tqdm(desc='Loading 2grams', position=0, leave=True, total=59134224)\n",
    "    with open(path, 'r') as f:\n",
    "        row = True\n",
    "        while row:\n",
    "            row = f.readline().strip()\n",
    "\n",
    "            pbar.update(1)\n",
    "            row = row.split()\n",
    "\n",
    "            if len(row) == 0:\n",
    "                continue\n",
    "\n",
    "            cnt, word, following = int(row[0]), row[1], row[2]\n",
    "\n",
    "            if cnt < MIN_N:\n",
    "                continue\n",
    "\n",
    "            word_mapping[word].add(following)\n",
    "            word_mapping_count[(word, following)] += cnt\n",
    "\n",
    "            if word not in word_to_tag_mapping or following not in word_to_tag_mapping:\n",
    "                continue\n",
    "\n",
    "            tag_word, tag_following = word_to_tag_mapping[word], word_to_tag_mapping[following]\n",
    "            tag_mapping[tag_word].add(tag_following)\n",
    "            tag_mapping_count[(tag_word, tag_following)] += cnt\n",
    "           \n",
    "        pbar.close()\n",
    "    \n",
    "    for word in tqdm(list(word_mapping.keys()), desc='Normalizing mappings', position=0, leave=True):\n",
    "        word_sum = sum([word_mapping_count[(word, following)] for following in word_mapping[word]])\n",
    "        for following in word_mapping[word]:\n",
    "            word_mapping_count[(word, following)] /= word_sum\n",
    "        \n",
    "        if word not in word_to_tag_mapping:\n",
    "            continue\n",
    "        \n",
    "        tag_word = word_to_tag_mapping[word]\n",
    "        tag_sum = sum([tag_mapping_count[(tag_word, tag_following)] for tag_following in tag_mapping[tag_word]])\n",
    "        if tag_sum < 2:\n",
    "            continue\n",
    "        for tag_following in tag_mapping[tag_word]:\n",
    "            tag_mapping_count[(tag_word, tag_following)] /= tag_sum        \n",
    "        \n",
    "    return word_mapping, word_mapping_count, tag_mapping, tag_mapping_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 1. (3+1+Xp) \n",
    "Napisz program, który ustala autora zdania. Autorów jest trójka: Prus, Orzeszkowa, Sienkiewicz (POS), na SKOSie znajdziesz odpowiednie dane uczące. Powinieneś je podzielić na część uczącą i walidacyjną. Część X punktacji zależeć będzie od tego, jak Twój program wypadnie na tle innych programów dla danych testowych (które pojawią się później na SKOSie). Dwie podstawowe metody są następujące:\n",
    "\n",
    "a) Naive Bayes (będzie na wykładzie 4, powinieneś użyć przynajmniej jednej cechy, która nie jest słowem)\n",
    "\n",
    "b) Utworzenie trzech modeli językowych i wybór tego, który daje najepszy wynik na klasyfikowanym zdaniu.\n",
    "\n",
    "Za sprawdzenie dwóch podejść jest punkt premiowy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_author(text: str, occurence_dicts: Dict[Author, Dict[str, float]]) -> Author:\n",
    "    scores: List[float] = []\n",
    "    for author, occurence_dict in occurence_dicts.items():\n",
    "        words = preprocess(text=text).split()\n",
    "        score = np.sum(np.log([occurence_dict[word] + 1e-5 for word in words]))\n",
    "        scores.append(score)\n",
    "        \n",
    "    author = list(occurence_dicts.keys())[np.argmax(scores)]\n",
    "    return author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_classifier(folder_path: str) -> np.ndarray:\n",
    "    '''\n",
    "    Returns confussion matrix\n",
    "    '''\n",
    "    occurence_dicts = create_occurence_dicts()\n",
    "    \n",
    "    authors_number = len(list(occurence_dicts.keys()))\n",
    "    confussion_matrix = np.zeros((authors_number, authors_number))\n",
    "    \n",
    "    for relative_path in os.listdir(folder_path):\n",
    "        path = os.path.join(folder_path, relative_path)\n",
    "        \n",
    "        if not os.path.isfile(path):\n",
    "            continue\n",
    "        \n",
    "        true_author: Optional[Author] = None\n",
    "        \n",
    "        if 'orzeszkowej' in relative_path:\n",
    "            true_author = Author.ORZESZKOWA\n",
    "        if 'prusa' in relative_path:\n",
    "            true_author = Author.PRUS\n",
    "        if 'sienkiewicza' in relative_path:\n",
    "            true_author = Author.SIENKIEWICZ\n",
    "        \n",
    "        if true_author is None:\n",
    "            print(f'Could not find a true author of the text! Filename: {relative_path}')\n",
    "            continue\n",
    "        \n",
    "        text = open(path, 'r').read()\n",
    "        classified_author = find_author(text=text, occurence_dicts=occurence_dicts)\n",
    "        \n",
    "        confussion_matrix[true_author.value, classified_author.value] += 1\n",
    "        \n",
    "    return confussion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['PRUS', 'SIENKIEWICZ', 'ORZESZKOWA'],\n",
       " array([[21.,  0.,  0.],\n",
       "        [15., 12.,  0.],\n",
       "        [ 0.,  0., 12.]]),\n",
       " 'Accuracy: 0.75')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confussion_matrix = test_classifier(folder_path='testy1.nogit')\n",
    "authors = list(map(lambda author: author.name, list(Author)))\n",
    "authors, confussion_matrix, f'Accuracy: {np.diag(confussion_matrix).sum() / confussion_matrix.sum()}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 2. (3+Xp) \n",
    "W zadaniu tym powinieneś uporządkować (spermutować) ciąg słów, żeby utworzył zdanie. W stosunku do poprzedniej listy mamy następujące różnice:\n",
    "\n",
    "a) powinieneś wykorzystać tagi słów (na przykład z pliku supertags.txt, link na SKOSie)\n",
    "\n",
    "b) powinieneś je połączyć ze zwykłymi statystykami bigramowymi (lub, opcjonalnie, z sufiksami)\n",
    "\n",
    "c) powinieneś wyodrębnić z danych uczących część walidacyjną i dobrać parametry łączenia modeli bazujących na słowach i bazujących na tagach.\n",
    "\n",
    "Sposób oceny będzie taki sam, jak w przypadku zadania z P1. Wybór zdań do oceny pojawi się przed zajęciami (będą to zdania zawierające od 4 do 8 tokenów, wybrane losowo z części testowej korpusu PolEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_tag_mapping = create_word_to_tag_mapping(path='data.nogit/supertags.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading 2grams: 59134225it [09:25, 104534.28it/s]                               \n",
      "Normalizing mappings: 100%|██████████| 1004769/1004769 [31:57<00:00, 524.01it/s] \n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('bigrams_saved/saved.pkl'):\n",
    "    d = pickle.load(open('bigrams_saved/saved.pkl', 'rb'))\n",
    "    word_bigrams, word_bigrams_count, tag_bigrams, tag_bigrams_count = \\\n",
    "        d['from_words']['bigrams'], d['from_words']['bigrams_count'], d['from_tags']['bigrams'], d['from_tags']['bigrams_count']\n",
    "\n",
    "else:\n",
    "    word_bigrams, word_bigrams_count, tag_bigrams, tag_bigrams_count = \\\n",
    "        load_bigrams(path='data.nogit/poleval_2grams.txt', word_to_tag_mapping=word_to_tag_mapping)\n",
    "\n",
    "    import pickle\n",
    "    to_save = {\n",
    "        'from_words': {'bigrams': word_bigrams, 'bigrams_count': word_bigrams_count},\n",
    "        'from_tags' : {'bigrams': tag_bigrams,  'bigrams_count': tag_bigrams_count},\n",
    "        }\n",
    "    pickle.dump(\n",
    "        to_save,\n",
    "        open('bigrams_saved/saved.pkl', 'wb+'),\n",
    "        pickle.HIGHEST_PROTOCOL,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentence_naturalness(sentence: List[str], alpha: float) -> Tuple[float, float, float]:\n",
    "    word_naturalness = tag_naturalness = overall_naturalness = 0\n",
    "    for i in range(1, len(sentence)):\n",
    "        word, following = sentence[i-1:i+1]\n",
    "        word_naturalness += word_bigrams_count[(word, following)]\n",
    "        \n",
    "        if word not in word_to_tag_mapping or following not in word_to_tag_mapping:\n",
    "            continue\n",
    "        \n",
    "        tag_word, tag_following = word_to_tag_mapping[word], word_to_tag_mapping[following]\n",
    "        tag_naturalness += tag_bigrams_count[(tag_word, tag_following)]\n",
    "        \n",
    "        overall_naturalness = word_naturalness * alpha + tag_naturalness * (1 - alpha)\n",
    "        \n",
    "    return word_naturalness, tag_naturalness, overall_naturalness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_sentence(original_sentence: str, alpha: float, verbose: bool=False) -> Tuple[int, int, int]:\n",
    "    '''\n",
    "    Returns position of the original sentence.\n",
    "    '''\n",
    "    original_sentence = preprocess(original_sentence)\n",
    "    words = preprocess(original_sentence).split()\n",
    "    \n",
    "    permutations_list = list(permutations(words))\n",
    "    naturalness_array = np.zeros((len(permutations_list), 3))  # in each row - naturalness from words, naturalness from tags, and overall one\n",
    "    \n",
    "    for i, sentence in enumerate(permutations_list):\n",
    "        naturalness_array[i] = list(compute_sentence_naturalness(sentence=sentence, alpha=alpha))\n",
    "    \n",
    "    best_for_word_naturalness = np.argmax(naturalness_array[:, 0])\n",
    "    best_for_tag_naturalness = np.argmax(naturalness_array[:, 1])\n",
    "    best_overall_naturalness = np.argmax(naturalness_array[:, 2])\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Best from word bigrams:\\t{\" \".join(permutations_list[best_for_word_naturalness])}')\n",
    "        print(f'Best from tag bigrams:\\t{\" \".join(permutations_list[best_for_tag_naturalness])}')\n",
    "        print(f'Best overall:\\t{\" \".join(permutations_list[best_overall_naturalness])}')\n",
    "    \n",
    "    idx_of_original = np.flatnonzero(np.array([' '.join(perm) for perm in permutations_list]) == original_sentence)[0]\n",
    "    \n",
    "    idx_from_word = np.flatnonzero(np.argsort(-naturalness_array[:, 0]) == idx_of_original)[0]\n",
    "    idx_from_tag = np.flatnonzero(np.argsort(-naturalness_array[:, 1]) == idx_of_original)[0]\n",
    "    idx_from_overall = np.flatnonzero(np.argsort(-naturalness_array[:, 2]) == idx_of_original)[0]\n",
    "    \n",
    "    return idx_from_word, idx_from_tag, idx_from_overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_reconstructing():\n",
    "    sentences = open('test_reconstruct_sents.txt', 'r').read().split('\\n')\n",
    "    acc = np.zeros((len(sentences), 3))\n",
    "    for i, sentence in tqdm(enumerate(sentences), position=0, leave=True):\n",
    "        words_rec, tags_rec, overall_rec = reconstruct_sentence(original_sentence=sentence, alpha=0.4)\n",
    "        acc[i] = [words_rec, tags_rec, overall_rec]\n",
    "    \n",
    "    print(f'From word bigrams:\\nscore: {np.mean(1 / (acc[:, 0] + 1))}\\ngood ones: {np.count_nonzero(acc[:, 0] == 0)}\\n')\n",
    "    print(f'From tag bigrams:\\nscore: {np.mean(1 / (acc[:, 1] + 1))}\\ngood ones: {np.count_nonzero(acc[:, 1] == 0)}\\n')\n",
    "    print(f'From overall:\\nscore: {np.mean(1 / (acc[:, 2] + 1))}\\ngood ones: {np.count_nonzero(acc[:, 2] == 0)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [00:02, 21.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From word bigrams:\n",
      "score: 0.23225804539576722\n",
      "good ones: 12\n",
      "\n",
      "From tag bigrams:\n",
      "score: 0.25256706740700174\n",
      "good ones: 12\n",
      "\n",
      "From overall:\n",
      "score: 0.2632054478187619\n",
      "good ones: 12\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_reconstructing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 3. (4p) \n",
    "W zadaniu tym powinieneś losować zdania o słowach z identyczną charakterystyką gramatyczną jak zdanie wejściowe). Przykładowo dla zdania:\n",
    "\n",
    "`Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę.` \n",
    "wynikiem mogłoby być\n",
    "Gruby Stefan przeczytał we wczorajszej gazecie starannie przygotowaną analizę.\n",
    "Zgodność gramatyczną sprawdzamy za pomocą tagów z pliku supertags. Przyjmijmy, że słowo s niewystępujące w tym pliku ma opis gramatyczny (’^’ + s)[-3:]. Powinieneś korzystać ze statystyk unigramowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tag_to_words_mapping() -> DefaultDict[str, List[str]]:\n",
    "    tag_to_words_mapping = defaultdict(list)\n",
    "    \n",
    "    for word, tag in tqdm(word_to_tag_mapping.items(), desc='Creating tag to words mapping', position=0, leave=True):\n",
    "        tag_to_words_mapping[tag].append(word)\n",
    "    return tag_to_words_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = pickle.load(open('data.nogit/corpora_counts.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating tag to words mapping: 100%|██████████| 1781994/1781994 [00:25<00:00, 71117.19it/s] \n"
     ]
    }
   ],
   "source": [
    "tag_to_words_mapping = create_tag_to_words_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similar_sentence(original_sentence: str) -> str:\n",
    "    result = ''\n",
    "    preprocessed = preprocess(original_sentence)\n",
    "    original_words = preprocessed.split()\n",
    "    \n",
    "    for word in original_words:\n",
    "        if word in word_to_tag_mapping:\n",
    "            tag = word_to_tag_mapping[word]\n",
    "        else:\n",
    "            tag = word_to_tag_mapping['^'+s[-3:]]\n",
    "        \n",
    "        possible_words = tag_to_words_mapping[tag]\n",
    "        probs = np.array([word_counts[possible_word] for possible_word in possible_words])\n",
    "        probs /= probs.sum()\n",
    "        next_word = np.random.choice(possible_words, p=probs)\n",
    "        result += next_word + ' '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "orignal_sentences = [\n",
    "    'Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę.',\n",
    "    'Pani postanowiła przejść się po niewielkim parku.',\n",
    "    'Niemiecki lekarz prowadzi sportowy samochód.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę.\n",
      "\n",
      "sympatyczny poprzednik utworzył pod nierdzewnej drużynie prawdopodobnie prowadzoną eksploatację \n",
      "piękny nabywca polecił na niepodległościowej galaktyce stosownie wzbogaconą rolę \n",
      "obywatelski organizator zaliczył w niewielkiej padlinie znakomicie przyznaną motywację \n",
      "\n",
      " ------------------------------------------ \n",
      "\n",
      "Original sentence: Pani postanowiła przejść się po niewielkim parku.\n",
      "\n",
      "pani rozpoczęła przejść się na niebywałym parku \n",
      "pani zajęła zdobyć się na niebezpiecznym parku \n",
      "pani doprowadziła zdobyć się w niezależnym parku \n",
      "\n",
      " ------------------------------------------ \n",
      "\n",
      "Original sentence: Niemiecki lekarz prowadzi sportowy samochód.\n",
      "\n",
      "nowy poseł mówi wolny projekt \n",
      "wąski prokurator kocha prawdziwy awans \n",
      "narodowy pan przyznaje umiarkowany udział \n",
      "\n",
      " ------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for original_sentence in orignal_sentences:\n",
    "    print(f'Original sentence: {original_sentence}\\n')\n",
    "    for _ in range(3):\n",
    "        print(create_similar_sentence(original_sentence))\n",
    "    print('\\n', '-'*42, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 4. (3p) \n",
    "Dodaj statystyki bigramowe do powyższego zadania. Postaraj się, by jak najrzadziej zdażały się sytuacje, w których musisz losować posługując się unigramami. Zaznaczaj znakiem \"|\" każdą taką nieciągłość."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_similar_sentence_with_bigram(original_sentence: str) -> str:\n",
    "    result = ''\n",
    "    preprocessed = preprocess(original_sentence)\n",
    "    original_words = preprocessed.split()\n",
    "    \n",
    "    previous_word = None\n",
    "    \n",
    "    for word in original_words:\n",
    "        if word in word_to_tag_mapping:\n",
    "            tag = word_to_tag_mapping[word]\n",
    "        else:\n",
    "            tag = word_to_tag_mapping['^'+s[-3:]]\n",
    "        \n",
    "        if previous_word is None:  # first word\n",
    "            possible_words = tag_to_words_mapping[tag]\n",
    "        else:\n",
    "            possible_words = [word for word in tag_to_words_mapping[tag] if word in word_bigrams[previous_word]]\n",
    "            \n",
    "            if len(possible_words) == 0:\n",
    "                result += ' | '\n",
    "                possible_words = tag_to_words_mapping[tag]\n",
    "                \n",
    "        \n",
    "        probs = np.array([word_counts[possible_word] for possible_word in possible_words])\n",
    "        probs /= probs.sum()\n",
    "        next_word = np.random.choice(possible_words, p=probs)\n",
    "        result += next_word + ' '\n",
    "        \n",
    "        previous_word = word\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Mały Piotruś spotkał w niewielkiej restauracyjce wczoraj poznaną koleżankę.\n",
      "\n",
      "górny pan  | przyjął w niewielkiej górze  | około  | prognozowaną historię \n",
      "barski pan  | zyskał w nieuczciwej zmianie  | obiektywnie  | oznaczoną historię \n",
      "główny jan  | przekazał na niedostatecznej stracie  | wielce  | zabrudzoną historię \n",
      "\n",
      " ------------------------------------------ \n",
      "\n",
      "Original sentence: Pani postanowiła przejść się po niewielkim parku.\n",
      "\n",
      "pani skończyła zdobyć się w niezwykłym parku \n",
      "pani podjęła przejść się na niezbędnym parku \n",
      "pani powołała pozbyć się w niespodziewanym parku \n",
      "\n",
      " ------------------------------------------ \n",
      "\n",
      "Original sentence: Niemiecki lekarz prowadzi sportowy samochód.\n",
      "\n",
      "demokratyczny rzecznik podchodzi szeroki projekt \n",
      "właściwy minister określa każdy tłumik \n",
      "spektakularny malarz podkreśla inny projekt \n",
      "\n",
      " ------------------------------------------ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for original_sentence in orignal_sentences:\n",
    "    print(f'Original sentence: {original_sentence}\\n')\n",
    "    for _ in range(3):\n",
    "        print(create_similar_sentence_with_bigram(original_sentence))\n",
    "    print('\\n', '-'*42, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 5. (5+1p) \n",
    "W zadaniu tym zajmiemy się kolokacjami słów, o których mamy informacje gramatyczną. Napisz program, który dla danego słowa znajduje k najbardziej z nim „spokrewnio- nych” słów. Rozważ następujące metody wyznaczania kolokacji:\n",
    "\n",
    "a) PPMI (Positive Pointwise Mutual Information)\n",
    "\n",
    "b) jakiś inny, dowolnie wybrany, z używanych na kolokacje wzorów (więcej na wykładzie),\n",
    "\n",
    "c) kolokacje gramatyczno-słowowe (tzn. żeby dwa słowa były uznane za kolokacje, warunek kolo- kacyjności (dowolnie wybrany) powinny spełniać zarówno tagi słów, jak i same słowa.\n",
    "\n",
    "d) jakaś dowolna inna metoda, lub Twoja modyfikacja powyższych (za to dodatkowy punkt).\n",
    "\n",
    "Możesz się ograniczyć do słów, które są stosunkowo częste (więcej niż n wystąpień w korpusie) i występują co najmniej raz w jakimś trigramie (lub k razy w jakimś bigramie). Wybierz niewielki zbiór słów (powiedzmy koło 10). Przygotuj raport, w którym dla każdego z tych słów jest 10 najbardziej spokrewnionych słow (czyli takich, o największym współczynniku kolokacji), dla różnych metod wyznaczania kolokacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 6. (7p) \n",
    "W zadaniu tym będziemy tworzyć pierwszą wersję programu tworzącego poezję (przypominającą Pana Tadeusza, oznaczanego dalej PT)). Przypomnijmy najważniejsze fakty od- noszące się do tego utworu (i ogólnie Poezji):\n",
    "\n",
    "F1. Wiersz składa się z wersetów, z których każdy ma ustaloną liczbę sylab (w PT to 13)\n",
    "\n",
    "F2. Ostatnie słowo w wersecie n rymuje się z ostatnim słowem w wersecie n + 1 (dla n parzystego,\n",
    "numeracja od 0).\n",
    "\n",
    "F3. Rym to zgodność ostatniej sylaby i części od samogłoski sylaby przedostatniej (zdrowie-dowie). W zasadzie rymy to zjawisko fonetyczne, ale dla języka polskiego (w pierwszej wersji) można sobie to trochę uprościć i powiedzieć, że dotyczą one liter.\n",
    "\n",
    "F4. Akcenty słów i podziały słów muszą się jakoś sensownie układać. Nam wystarczy przyjąć, że godzimy się jedynie na takie podziały wersu na słowa k-sylabowe3, których użył Adam Mickiewicz, na przykład:\n",
    "\n",
    "Litwo, Ojczyzno moja, Ty jesteś jak zdrowie, ile cię trzeba cenić, ten tylko się dowie ma schemat: [2,3,2,1,2,1,2] -- [2,1,2,2,1,2,1,2]\n",
    "\n",
    "F5. Podział na sylaby nie jest trywialny (dlaczego?). Ale na nasze szczęście policzenie sylab jest łatwe. Słowo ma tyle sylab, ile ma samogłosek (przy czym połączenia ie, iu, ię, itd traktujemy jako jedną samogłoskę). Część rymowana wyrazu to część wyrazu od przedostatniej samogłoski do końca.\n",
    "\n",
    "Powinieneś stworzyć program, generujący dwuwersowe fragmenty wierszy w stylu PT, czyli powinieneś przypilnować:\n",
    "\n",
    "a) żeby wersy były poprawne rytmicznie i się rymowały,\n",
    "\n",
    "b) żeby dwuwers miał sens gramatyczny (czyli by tagi słów pasowały do jakiegoś zdania lub frag- mentu zdania)\n",
    "\n",
    "c) żeby były jakoś wykorzystane statystyki N-gramowe (plan minimum to statystyki 1-gramowe, dodatkowe +1 za wykorzystanie bigramów)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zadanie 7. (5p) \n",
    "Zmodyfikuj algorytm z poprzedniego zadania w ten sposób, by starał się on maksymalizować wzajemną „kolokacyjność” słów z dwuwersu (tak, by jak najwięcej słów było ze sobą powiązanych, na przykład przez wysokie PPMI). Akceptowalna jest dowolna procedura (local search, jakieś błądzenie losowe, metody ewolucyjne, ...), która daje wartość liczby par słów będących kolokacjami istotnie większą niż losowanie z poprzedniego zadania.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
